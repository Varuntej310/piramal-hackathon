{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8568e232-dec5-4aef-b7e1-e2512f96803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e047bcdf-1013-47fc-87e7-55abce3500d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777584dc-c744-4d27-8bbd-5aa60e82448f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading geopy-2.4.1-py3-none-any.whl (125 kB)\n",
      "     ------------------------------------ 125.4/125.4 kB 184.3 kB/s eta 0:00:00\n",
      "Collecting geographiclib<3,>=1.52\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 40.3/40.3 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eeafadb-ad20-468c-92f6-04fa98f48733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "import requests\n",
    "\n",
    "def get_coordinates(pincode):\n",
    "    url = f'https://nominatim.openstreetmap.org/search?q={pincode}&format=json&addressdetails=1'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if len(data) > 0:\n",
    "        return data[0]['lat'], data[0]['lon']\n",
    "    else:\n",
    "        return '0', '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ede1b7-6412-406b-b97f-c3558f97beb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CandidateID                                                                      0\n",
       "Designation                                                                      0\n",
       "Have you Completed your Graduation ?                                           101\n",
       "Highest Educational Qualification                                              101\n",
       "Total no of years Experience [before joining Piramal]                           74\n",
       "Previous Industry worked with [before joining Piramal]                           0\n",
       "Name of your Previous Organization / Company                                   192\n",
       "How many Organization that you have worked before joining Piramal Finance ?     74\n",
       "Average Incentive [per month] earned in your pervious company ?                  0\n",
       "How did you come to know about the role at Piramal Finance ?                     0\n",
       "Which Products you are selling in your pervious role ?                           6\n",
       "What was the average ticket size handled at your end in previous role ?          0\n",
       "How many members are there in your family ?                                      0\n",
       "How many are earning family members ? [Other then yourself]2                   182\n",
       "How many members are dependent on you ?                                          0\n",
       "Department                                                                       0\n",
       "DOJ                                                                              0\n",
       "Location Code                                                                    0\n",
       "Residential Pincode                                                              0\n",
       "Branch Pincode                                                                   2\n",
       "Performance                                                                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafa7363-1c2b-44df-b2a5-ebe672f3d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col8 = 'Branch Pincode'\n",
    "col9 = 'Residential Pincode'\n",
    "train[col8] = train[col8].fillna(train[col9])\n",
    "test[col8] = test[col8].fillna(train[col9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8273be60-9b95-4f35-ba62-38bffe980429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6428a4d-5df4-4a03-83ac-c1f947855c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting...:  57%|██████████████████████████████████████▍                            | 428/745 [14:59<11:06,  2.10s/it]\n"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=500013&format=json&addressdetails=1 (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLEOFError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1058\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1058\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    417\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m send_sni:\n\u001b[1;32m--> 449\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    508\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    510\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1375\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1375\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mSSLEOFError\u001b[0m: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:799\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    797\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 799\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=500013&format=json&addressdetails=1 (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m lon_rl \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm (\u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m745\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCounting...\u001b[39m\u001b[38;5;124m\"\u001b[39m) :\n\u001b[1;32m----> 4\u001b[0m     lat_r, lon_r \u001b[38;5;241m=\u001b[39m \u001b[43mget_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mResidential Pincode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      5\u001b[0m     lat_rl\u001b[38;5;241m.\u001b[39mappend(lat_r)\n\u001b[0;32m      6\u001b[0m     lon_rl\u001b[38;5;241m.\u001b[39mappend(lon_r)\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m, in \u001b[0;36mget_coordinates\u001b[1;34m(pincode)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coordinates\u001b[39m(pincode):\n\u001b[0;32m      5\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://nominatim.openstreetmap.org/search?q=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpincode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&format=json&addressdetails=1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:517\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mSSLError\u001b[0m: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=500013&format=json&addressdetails=1 (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))"
     ]
    }
   ],
   "source": [
    "lat_rl = []\n",
    "lon_rl = []\n",
    "for i in tqdm (range (745), desc=\"Counting...\") :\n",
    "    lat_r, lon_r = get_coordinates(train['Residential Pincode'][i]) \n",
    "    lat_rl.append(lat_r)\n",
    "    lon_rl.append(lon_r)\n",
    "len(lat_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56b4c9c6-f31d-44e1-a33d-cb507f173ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting...: 100%|███████████████████████████████████████████████████████████████████| 745/745 [21:41<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "lat_bl = []\n",
    "lon_bl = []\n",
    "for i in tqdm (range (745), desc=\"Counting...\") :\n",
    "    lat_b, lon_b = get_coordinates(train['Branch Pincode'][i]) \n",
    "    lat_bl.append(lat_b)\n",
    "    lon_bl.append(lon_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b56a02-b56a-4a58-9692-718cd1798d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting...: 100%|███████████████████████████████████████████████████████████████████| 187/187 [05:03<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "lat_rlt = []\n",
    "lon_rlt = []\n",
    "for i in tqdm (range (187), desc=\"Counting...\") :\n",
    "    lat_r, lon_r = get_coordinates(test['Residential Pincode'][i]) \n",
    "    lat_rlt.append(lat_r)\n",
    "    lon_rlt.append(lon_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17ddd4cc-e611-4998-9096-bc85bde3c1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting...: 100%|███████████████████████████████████████████████████████████████████| 187/187 [05:31<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "lat_blt = []\n",
    "lon_blt = []\n",
    "for i in tqdm (range (187), desc=\"Counting...\") :\n",
    "    lat_b, lon_b = get_coordinates(test['Branch Pincode'][i]) \n",
    "    lat_blt.append(lat_b)\n",
    "    lon_blt.append(lon_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be9e2fd2-c17e-45df-b968-fe845b5c1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['branch_lat'] = pd.DataFrame(lat_bl, columns=['branch_lat'])\n",
    "train['branch_lon'] = pd.DataFrame(lon_bl, columns=['branch_lon'])\n",
    "train['residential_lat'] = pd.DataFrame(lat_rl, columns=['residential_lat'])\n",
    "train['residential_lon'] = pd.DataFrame(lon_rl, columns=['residential_lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d643b904-cde8-4ec0-b893-e3f32a58c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['branch_lat'] = pd.DataFrame(lat_blt, columns=['branch_lat'])\n",
    "test['branch_lon'] = pd.DataFrame(lon_blt, columns=['branch_lon'])\n",
    "test['residential_lat'] = pd.DataFrame(lat_rlt, columns=['residential_lat'])\n",
    "test['residential_lon'] = pd.DataFrame(lon_rlt, columns=['residential_lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7eda8cd-1d41-48c7-ac33-78bcfb08b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"lat_bl.pkl\", \"wb\") as file:\n",
    "    pickle.dump(lat_bl, file)\n",
    "\n",
    "# with open(\"lon_bl.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(lon_bl, file)\n",
    "\n",
    "# with open(\"lat_rl.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(lat_rl, file)\n",
    "\n",
    "# with open(\"lon_rl.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(lon_rl, file)\n",
    "\n",
    "# with open(\"lat_blt.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(lat_blt, file)\n",
    "\n",
    "# with open(\"lon_blt.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(lon_blt, file)\n",
    "\n",
    "# with open(\"lat_rlt.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(lat_rlt, file)\n",
    "\n",
    "# with open(\"lon_rlt.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(lon_rlt, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "277a7284-bdc5-4345-9a16-0ac06f0de1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lat_bl.pkl\", \"rb\") as file:\n",
    "    lat_bl = pickle.load(file)\n",
    "\n",
    "# with open(\"lon_bl.pkl\", \"rb\") as file:\n",
    "#     lon_bl = pickle.load(file)\n",
    "\n",
    "# with open(\"lat_rl.pkl\", \"rb\") as file:\n",
    "#     lat_rl = pickle.load(file)\n",
    "\n",
    "# with open(\"lon_rl.pkl\", \"rb\") as file:\n",
    "#     lon_rl = pickle.load(file)\n",
    "\n",
    "# with open(\"lat_blt.pkl\", \"rb\") as file:\n",
    "#     lat_blt = pickle.load(file)\n",
    "\n",
    "# with open(\"lon_blt.pkl\", \"rb\") as file:\n",
    "#     lon_blt = pickle.load(file)\n",
    "\n",
    "# with open(\"lat_rlt.pkl\", \"rb\") as file:\n",
    "#     lat_rlt = pickle.load(file)\n",
    "\n",
    "# with open(\"lon_rlt.pkl\", \"rb\") as file:\n",
    "#     lon_rlt = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe125093-e451-4872-a417-26b976a37656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    if (pd.notna(lat1) and pd.notna(lon1) and pd.notna(lat2) and pd.notna(lon2)):\n",
    "        point1 = (lat1, lon1)\n",
    "        point2 = (lat2, lon2)\n",
    "        return geodesic(point1, point2).km\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "train['distance'] = train.apply(lambda row: calculate_distance(row['branch_lat'], row['branch_lon'], row['residential_lat'], row['residential_lon']), axis=1)\n",
    "test['distance'] = test.apply(lambda row: calculate_distance(row['branch_lat'], row['branch_lon'], row['residential_lat'], row['residential_lon']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f686b9f-31d2-401c-932f-34b9dfc291ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_dist.csv', index=False)\n",
    "test.to_csv('test_dist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22074c6c-d543-4eea-bb95-41e4fddd44f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Have you Completed your Graduation ?'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Have you Completed your Graduation ?'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meducation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHave you Completed your Graduation ?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHighest Educational Qualification\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHave you Completed your Graduation ?\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHighest Educational Qualification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbranch_lat\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbranch_lon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresidential_lat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresidential_lon\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meducation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHave you Completed your Graduation ?\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHighest Educational Qualification\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Have you Completed your Graduation ?'"
     ]
    }
   ],
   "source": [
    "train['education'] = train['Have you Completed your Graduation ?'] + ' ' + train['Highest Educational Qualification']\n",
    "train = train.drop(['Have you Completed your Graduation ?', 'Highest Educational Qualification', 'branch_lat','branch_lon', 'residential_lat', 'residential_lon'], axis=1)\n",
    "test['education'] = test['Have you Completed your Graduation ?'] + ' ' + test['Highest Educational Qualification']\n",
    "test = test.drop(['Have you Completed your Graduation ?', 'Highest Educational Qualification', 'branch_lat','branch_lon', 'residential_lat', 'residential_lon'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be9eef-09e9-4f01-9501-b6ba6ccd53c0",
   "metadata": {},
   "source": [
    ", 'branch_lat','branch_lon', 'residential_lat', 'residential_lon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "131229a3-acf4-45a7-a710-ac8256a59cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resume_data_new.pkl\", \"rb\") as file:\n",
    "    resume_text_new = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eead721c-8d26-417e-bf66-af0aa61ebc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume_txt = pd.DataFrame(resume_text, columns=['resume_txt'])\n",
    "# df = pd.concat([train, resume_txt], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f5c69f-3910-4cee-ae3f-f279031c5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "with open(\"resume_data_test_new1.pkl\", \"rb\") as file:\n",
    "    resume_text_test_new1 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b96736d-656d-4db2-b77e-4eeff1526690",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resume_data_test_new2.pkl\", \"rb\") as file:\n",
    "    resume_text_test_new2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c59c25c-1436-4c8b-bf7b-7b60aae94d77",
   "metadata": {},
   "outputs": [],
   "source": [
    " resume_text_test_new =  resume_text_test_new1 +  resume_text_test_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07d0fab1-7f18-4f88-ae2d-440f98447d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 745/745 [00:17<00:00, 41.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 187/187 [00:04<00:00, 42.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import docx2txt\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm import tqdm\n",
    "\n",
    "# # Load the training and test data\n",
    "# train_dir = 'CV train'\n",
    "# test_dir = 'CV test'\n",
    "\n",
    "# # Load the training data\n",
    "# train_docs = []\n",
    "# for filename in os.listdir(train_dir):\n",
    "#     if filename.endswith('.docx'):\n",
    "#         file_path = os.path.join(train_dir, filename)\n",
    "#         text = docx2txt.process(file_path)\n",
    "#         train_docs.append(text)\n",
    "\n",
    "# # Load the test data\n",
    "# test_docs = []\n",
    "# for filename in os.listdir(test_dir):\n",
    "#     if filename.endswith('.docx'):\n",
    "#         file_path = os.path.join(test_dir, filename)\n",
    "#         text = docx2txt.process(file_path)\n",
    "#         test_docs.append(text)\n",
    "\n",
    "# Convert the text to tagged documents\n",
    "train_tagged = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(resume_text_new)]\n",
    "test_tagged = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(resume_text_test_new)]\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "model = Doc2Vec(vector_size=700, min_count=2, epochs=40)\n",
    "model.build_vocab(train_tagged)\n",
    "model.train(train_tagged, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Get the document vectors\n",
    "train_vectors = [model.infer_vector(doc.words) for doc in tqdm(train_tagged)]\n",
    "test_vectors = [model.infer_vector(doc.words) for doc in tqdm(test_tagged)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137fea2d-559c-4e67-9558-da35d075ad72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffea0f6b-904a-4349-9091-378fef4efcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['train_vector'] = train_vectors\n",
    "test['train_vector'] = test_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68b17e74-99e3-4a6d-9a9d-cfd07baf7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = test.rename(columns = {'test_vector': 'train_vector'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef73af7b-a7e4-4874-88c3-e79ed5c7dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resume_test = pd.DataFrame(resume_text_test, columns = ['resume_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "216ecc4f-d78d-47e4-96de-d740ff2afba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.concat([test, resume_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5ce5a8b-b914-4332-b13b-5847c6a35af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train1.csv', index=False)\n",
    "test.to_csv('test1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1023077d-a63e-4fd2-af2c-fb87da3fbaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: train1.csv | Columns = 21 / 21 | Rows = 745 -> 745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['CandidateID', 'Designation',\n",
       "       'Total no of years Experience [before joining Piramal]',\n",
       "       'Previous Industry worked with [before joining Piramal]',\n",
       "       'Name of your Previous Organization / Company',\n",
       "       'How many Organization that you have worked before joining Piramal Finance ?',\n",
       "       'Average Incentive [per month] earned in your pervious company ?',\n",
       "       'How did you come to know about the role at Piramal Finance ?',\n",
       "       'Which Products you are selling in your pervious role ?',\n",
       "       'What was the average ticket size handled at your end in previous role ?',\n",
       "       'How many members are there in your family ?',\n",
       "       'How many are earning family members ? [Other then yourself]2',\n",
       "       'How many members are dependent on you ?', 'Department', 'DOJ',\n",
       "       'Location Code', 'Residential Pincode', 'Branch Pincode', 'Performance',\n",
       "       'education', 'train_vector'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.core.dataset import TabularDataset\n",
    "train_data = TabularDataset('train1.csv')\n",
    "train_data.head(30)\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fab7b34-50e3-4550-a510-32801362d6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240411_093252\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240411_093252\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.14\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       5.04 GB / 15.63 GB (32.2%)\n",
      "Disk Space Avail:   111.10 GB / 365.58 GB (30.4%)\n",
      "===================================================\n",
      "Train Data Rows:    745\n",
      "Train Data Columns: 20\n",
      "Label Column:       Performance\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5168.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.52 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Name of your Previous Organization / Company', 'Which Products you are selling in your pervious role ?', 'What was the average ticket size handled at your end in previous role ?', 'education', 'train_vector']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 143\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 143 to 111 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['Designation']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['CandidateID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['CandidateID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 3 | ['Total no of years Experience [before joining Piramal]', 'How many are earning family members ? [Other then yourself]2', 'Branch Pincode']\n",
      "\t\t('int', [])                        : 3 | ['How many members are dependent on you ?', 'Location Code', 'Residential Pincode']\n",
      "\t\t('object', [])                     : 6 | ['Previous Industry worked with [before joining Piramal]', 'How many Organization that you have worked before joining Piramal Finance ?', 'Average Incentive [per month] earned in your pervious company ?', 'How did you come to know about the role at Piramal Finance ?', 'How many members are there in your family ?', ...]\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['DOJ']\n",
      "\t\t('object', ['text'])               : 5 | ['Name of your Previous Organization / Company', 'Which Products you are selling in your pervious role ?', 'What was the average ticket size handled at your end in previous role ?', 'education', 'train_vector']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  6 | ['Previous Industry worked with [before joining Piramal]', 'How many Organization that you have worked before joining Piramal Finance ?', 'Average Incentive [per month] earned in your pervious company ?', 'How did you come to know about the role at Piramal Finance ?', 'How many members are there in your family ?', ...]\n",
      "\t\t('category', ['text_as_category'])  :  4 | ['Name of your Previous Organization / Company', 'Which Products you are selling in your pervious role ?', 'What was the average ticket size handled at your end in previous role ?', 'education']\n",
      "\t\t('float', [])                       :  3 | ['Total no of years Experience [before joining Piramal]', 'How many are earning family members ? [Other then yourself]2', 'Branch Pincode']\n",
      "\t\t('int', [])                         :  3 | ['How many members are dependent on you ?', 'Location Code', 'Residential Pincode']\n",
      "\t\t('int', ['binned', 'text_special']) : 39 | ['Name of your Previous Organization / Company.char_count', 'Name of your Previous Organization / Company.word_count', 'Name of your Previous Organization / Company.capital_ratio', 'Name of your Previous Organization / Company.lower_ratio', 'Name of your Previous Organization / Company.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :  5 | ['DOJ', 'DOJ.year', 'DOJ.month', 'DOJ.day', 'DOJ.dayofweek']\n",
      "\t\t('int', ['text_ngram'])             : 69 | ['__nlp__.00', '__nlp__.01', '__nlp__.02', '__nlp__.03', '__nlp__.04', ...]\n",
      "\t44.0s = Fit runtime\n",
      "\t18 features in original data used to generate 129 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.20 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 44.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 596, Val Rows: 149\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.5586\t = Validation score   (f1)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.5664\t = Validation score   (f1)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.6087\t = Validation score   (f1)\n",
      "\t3.45s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.6281\t = Validation score   (f1)\n",
      "\t3.55s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.5051\t = Validation score   (f1)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.4946\t = Validation score   (f1)\n",
      "\t1.13s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.5688\t = Validation score   (f1)\n",
      "\t45.97s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.4444\t = Validation score   (f1)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.3864\t = Validation score   (f1)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "C:\\Users\\91939\\anaconda3\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "C:\\Users\\91939\\anaconda3\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "C:\\Users\\91939\\anaconda3\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "\t0.5968\t = Validation score   (f1)\n",
      "\t2.71s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.6422\t = Validation score   (f1)\n",
      "\t2.44s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.595\t = Validation score   (f1)\n",
      "\t9.57s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.6316\t = Validation score   (f1)\n",
      "\t4.65s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'XGBoost': 0.429, 'KNeighborsUnif': 0.286, 'LightGBMXT': 0.143, 'NeuralNetTorch': 0.143}\n",
      "\t0.7193\t = Validation score   (f1)\n",
      "\t4.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 127.0s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240411_093252\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor \n",
    "time_limit = 60\n",
    "predictor = TabularPredictor(label='Performance', eval_metric='f1').fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "056048a8-253b-41d9-a67f-64122ccccd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: test1.csv | Columns = 20 / 20 | Rows = 187 -> 187\n",
      "WARNING: Int features without null values at train time contain null values at inference time! Imputing nulls to 0. To avoid this, pass the features as floats during fit!\n",
      "WARNING: Int features with nulls: ['Location Code']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: Performance, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = TabularDataset('test1.csv')\n",
    "label = 'CandidateID'\n",
    "y_pred = predictor.predict((test_data))\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ee7dca1-07de-4b2e-84a0-5db110774ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CandidateID</th>\n",
       "      <th>Performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMP0521</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EMP0613</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EMP0136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EMP0351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EMP0049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>EMP0401</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>EMP0408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>EMP0248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>EMP0148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>EMP0422</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CandidateID  Performance\n",
       "0       EMP0521            1\n",
       "1       EMP0613            1\n",
       "2       EMP0136            0\n",
       "3       EMP0351            0\n",
       "4       EMP0049            0\n",
       "..          ...          ...\n",
       "182     EMP0401            1\n",
       "183     EMP0408            0\n",
       "184     EMP0248            0\n",
       "185     EMP0148            0\n",
       "186     EMP0422            1\n",
       "\n",
       "[187 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub=pd.DataFrame()\n",
    "sub['CandidateID'],sub['Performance']=test_data['CandidateID'],y_pred\n",
    "sub.to_csv('submission_6.csv', index=False)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc7b6be4-c02e-479a-b4d9-fc9969609428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['CandidateID', 'Designation']\n",
      "Computing feature importance via permutation shuffling for 16 features using 745 rows with 5 shuffle sets...\n",
      "C:\\Users\\91939\\anaconda3\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "\t58.2s\t= Expected runtime (11.64s per shuffle set)\n",
      "C:\\Users\\91939\\anaconda3\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "C:\\Users\\91939\\anaconda3\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "C:\\Users\\91939\\anaconda3\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "C:\\Users\\91939\\anaconda3\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "C:\\Users\\91939\\anaconda3\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "\t45.2s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DOJ</th>\n",
       "      <td>0.140403</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>5</td>\n",
       "      <td>0.164702</td>\n",
       "      <td>0.116103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Which Products you are selling in your pervious role ?</th>\n",
       "      <td>0.093423</td>\n",
       "      <td>0.010847</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>5</td>\n",
       "      <td>0.115756</td>\n",
       "      <td>0.071089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_vector</th>\n",
       "      <td>0.088322</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>5</td>\n",
       "      <td>0.107310</td>\n",
       "      <td>0.069334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What was the average ticket size handled at your end in previous role ?</th>\n",
       "      <td>0.053960</td>\n",
       "      <td>0.012207</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>5</td>\n",
       "      <td>0.079093</td>\n",
       "      <td>0.028826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name of your Previous Organization / Company</th>\n",
       "      <td>0.035168</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>5</td>\n",
       "      <td>0.045017</td>\n",
       "      <td>0.025318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Incentive [per month] earned in your pervious company ?</th>\n",
       "      <td>0.033826</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>5</td>\n",
       "      <td>0.043867</td>\n",
       "      <td>0.023784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>0.023624</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>5</td>\n",
       "      <td>0.035120</td>\n",
       "      <td>0.012129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department</th>\n",
       "      <td>0.019866</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>5</td>\n",
       "      <td>0.023469</td>\n",
       "      <td>0.016262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total no of years Experience [before joining Piramal]</th>\n",
       "      <td>0.013691</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>5</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.009150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How did you come to know about the role at Piramal Finance ?</th>\n",
       "      <td>0.012886</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021756</td>\n",
       "      <td>0.004016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location Code</th>\n",
       "      <td>0.009933</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014124</td>\n",
       "      <td>0.005741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How many members are there in your family ?</th>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>-0.000811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Previous Industry worked with [before joining Piramal]</th>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>-0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How many are earning family members ? [Other then yourself]2</th>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>-0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How many members are dependent on you ?</th>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.236714</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>-0.006475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How many Organization that you have worked before joining Piramal Finance ?</th>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.152279</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003957</td>\n",
       "      <td>-0.002346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             importance  \\\n",
       "DOJ                                                                            0.140403   \n",
       "Which Products you are selling in your pervious role ?                         0.093423   \n",
       "train_vector                                                                   0.088322   \n",
       "What was the average ticket size handled at your end in previous role ?        0.053960   \n",
       "Name of your Previous Organization / Company                                   0.035168   \n",
       "Average Incentive [per month] earned in your pervious company ?                0.033826   \n",
       "education                                                                      0.023624   \n",
       "Department                                                                     0.019866   \n",
       "Total no of years Experience [before joining Piramal]                          0.013691   \n",
       "How did you come to know about the role at Piramal Finance ?                   0.012886   \n",
       "Location Code                                                                  0.009933   \n",
       "How many members are there in your family ?                                    0.005369   \n",
       "Previous Industry worked with [before joining Piramal]                         0.002953   \n",
       "How many are earning family members ? [Other then yourself]2                   0.002953   \n",
       "How many members are dependent on you ?                                        0.001342   \n",
       "How many Organization that you have worked before joining Piramal Finance ?    0.000805   \n",
       "\n",
       "                                                                               stddev  \\\n",
       "DOJ                                                                          0.011801   \n",
       "Which Products you are selling in your pervious role ?                       0.010847   \n",
       "train_vector                                                                 0.009222   \n",
       "What was the average ticket size handled at your end in previous role ?      0.012207   \n",
       "Name of your Previous Organization / Company                                 0.004783   \n",
       "Average Incentive [per month] earned in your pervious company ?              0.004877   \n",
       "education                                                                    0.005583   \n",
       "Department                                                                   0.001750   \n",
       "Total no of years Experience [before joining Piramal]                        0.002206   \n",
       "How did you come to know about the role at Piramal Finance ?                 0.004308   \n",
       "Location Code                                                                0.002036   \n",
       "How many members are there in your family ?                                  0.003001   \n",
       "Previous Industry worked with [before joining Piramal]                       0.001750   \n",
       "How many are earning family members ? [Other then yourself]2                 0.001750   \n",
       "How many members are dependent on you ?                                      0.003797   \n",
       "How many Organization that you have worked before joining Piramal Finance ?  0.001530   \n",
       "\n",
       "                                                                              p_value  \\\n",
       "DOJ                                                                          0.000006   \n",
       "Which Products you are selling in your pervious role ?                       0.000021   \n",
       "train_vector                                                                 0.000014   \n",
       "What was the average ticket size handled at your end in previous role ?      0.000294   \n",
       "Name of your Previous Organization / Company                                 0.000040   \n",
       "Average Incentive [per month] earned in your pervious company ?              0.000050   \n",
       "education                                                                    0.000348   \n",
       "Department                                                                   0.000007   \n",
       "Total no of years Experience [before joining Piramal]                        0.000078   \n",
       "How did you come to know about the role at Piramal Finance ?                 0.001299   \n",
       "Location Code                                                                0.000200   \n",
       "How many members are there in your family ?                                  0.008065   \n",
       "Previous Industry worked with [before joining Piramal]                       0.009777   \n",
       "How many are earning family members ? [Other then yourself]2                 0.009777   \n",
       "How many members are dependent on you ?                                      0.236714   \n",
       "How many Organization that you have worked before joining Piramal Finance ?  0.152279   \n",
       "\n",
       "                                                                             n  \\\n",
       "DOJ                                                                          5   \n",
       "Which Products you are selling in your pervious role ?                       5   \n",
       "train_vector                                                                 5   \n",
       "What was the average ticket size handled at your end in previous role ?      5   \n",
       "Name of your Previous Organization / Company                                 5   \n",
       "Average Incentive [per month] earned in your pervious company ?              5   \n",
       "education                                                                    5   \n",
       "Department                                                                   5   \n",
       "Total no of years Experience [before joining Piramal]                        5   \n",
       "How did you come to know about the role at Piramal Finance ?                 5   \n",
       "Location Code                                                                5   \n",
       "How many members are there in your family ?                                  5   \n",
       "Previous Industry worked with [before joining Piramal]                       5   \n",
       "How many are earning family members ? [Other then yourself]2                 5   \n",
       "How many members are dependent on you ?                                      5   \n",
       "How many Organization that you have worked before joining Piramal Finance ?  5   \n",
       "\n",
       "                                                                             p99_high  \\\n",
       "DOJ                                                                          0.164702   \n",
       "Which Products you are selling in your pervious role ?                       0.115756   \n",
       "train_vector                                                                 0.107310   \n",
       "What was the average ticket size handled at your end in previous role ?      0.079093   \n",
       "Name of your Previous Organization / Company                                 0.045017   \n",
       "Average Incentive [per month] earned in your pervious company ?              0.043867   \n",
       "education                                                                    0.035120   \n",
       "Department                                                                   0.023469   \n",
       "Total no of years Experience [before joining Piramal]                        0.018233   \n",
       "How did you come to know about the role at Piramal Finance ?                 0.021756   \n",
       "Location Code                                                                0.014124   \n",
       "How many members are there in your family ?                                  0.011549   \n",
       "Previous Industry worked with [before joining Piramal]                       0.006557   \n",
       "How many are earning family members ? [Other then yourself]2                 0.006557   \n",
       "How many members are dependent on you ?                                      0.009159   \n",
       "How many Organization that you have worked before joining Piramal Finance ?  0.003957   \n",
       "\n",
       "                                                                              p99_low  \n",
       "DOJ                                                                          0.116103  \n",
       "Which Products you are selling in your pervious role ?                       0.071089  \n",
       "train_vector                                                                 0.069334  \n",
       "What was the average ticket size handled at your end in previous role ?      0.028826  \n",
       "Name of your Previous Organization / Company                                 0.025318  \n",
       "Average Incentive [per month] earned in your pervious company ?              0.023784  \n",
       "education                                                                    0.012129  \n",
       "Department                                                                   0.016262  \n",
       "Total no of years Experience [before joining Piramal]                        0.009150  \n",
       "How did you come to know about the role at Piramal Finance ?                 0.004016  \n",
       "Location Code                                                                0.005741  \n",
       "How many members are there in your family ?                                 -0.000811  \n",
       "Previous Industry worked with [before joining Piramal]                      -0.000651  \n",
       "How many are earning family members ? [Other then yourself]2                -0.000651  \n",
       "How many members are dependent on you ?                                     -0.006475  \n",
       "How many Organization that you have worked before joining Piramal Finance ? -0.002346  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.feature_importance(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a50efe68-6425-4f70-891e-3458d1a7fdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240410_200843\"\n",
      "Presets specified: ['high_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 60 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels\\ag-20240410_200843/ds_sub_fit/sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 22 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 38 seconds.\n",
      "Starting full fit now with num_stack_levels 1.\n",
      "Beginning AutoGluon training ... Time limit = 38s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240410_200843\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.14\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.72 GB / 15.63 GB (30.2%)\n",
      "Disk Space Avail:   106.89 GB / 365.58 GB (29.2%)\n",
      "===================================================\n",
      "Train Data Rows:    745\n",
      "Train Data Columns: 18\n",
      "Label Column:       Performance\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4839.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.59 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Name of your Previous Organization / Company', 'Which Products you are selling in your pervious role ?', 'What was the average ticket size handled at your end in previous role ?', 'education', 'train_vector']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 142\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 142 to 135 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['Designation']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['CandidateID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['CandidateID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 2 | ['Total no of years Experience [before joining Piramal]', 'How many are earning family members ? [Other then yourself]2']\n",
      "\t\t('int', [])                        : 2 | ['How many members are dependent on you ?', 'Location Code']\n",
      "\t\t('object', [])                     : 6 | ['Previous Industry worked with [before joining Piramal]', 'How many Organization that you have worked before joining Piramal Finance ?', 'Average Incentive [per month] earned in your pervious company ?', 'How did you come to know about the role at Piramal Finance ?', 'How many members are there in your family ?', ...]\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['DOJ']\n",
      "\t\t('object', ['text'])               : 5 | ['Name of your Previous Organization / Company', 'Which Products you are selling in your pervious role ?', 'What was the average ticket size handled at your end in previous role ?', 'education', 'train_vector']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  6 | ['Previous Industry worked with [before joining Piramal]', 'How many Organization that you have worked before joining Piramal Finance ?', 'Average Incentive [per month] earned in your pervious company ?', 'How did you come to know about the role at Piramal Finance ?', 'How many members are there in your family ?', ...]\n",
      "\t\t('category', ['text_as_category'])  :  4 | ['Name of your Previous Organization / Company', 'Which Products you are selling in your pervious role ?', 'What was the average ticket size handled at your end in previous role ?', 'education']\n",
      "\t\t('float', [])                       :  2 | ['Total no of years Experience [before joining Piramal]', 'How many are earning family members ? [Other then yourself]2']\n",
      "\t\t('int', [])                         :  2 | ['How many members are dependent on you ?', 'Location Code']\n",
      "\t\t('int', ['binned', 'text_special']) : 40 | ['Name of your Previous Organization / Company.char_count', 'Name of your Previous Organization / Company.word_count', 'Name of your Previous Organization / Company.capital_ratio', 'Name of your Previous Organization / Company.lower_ratio', 'Name of your Previous Organization / Company.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :  5 | ['DOJ', 'DOJ.year', 'DOJ.month', 'DOJ.day', 'DOJ.dayofweek']\n",
      "\t\t('int', ['text_ngram'])             : 87 | ['__nlp__.00', '__nlp__.01', '__nlp__.02', '__nlp__.03', '__nlp__.04', ...]\n",
      "\t8.6s = Fit runtime\n",
      "\t16 features in original data used to generate 146 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.22 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 8.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 19.57s of the 29.35s of remaining time.\n",
      "\t0.6389\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 19.4s of the 29.18s of remaining time.\n",
      "\t0.6322\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 19.24s of the 29.01s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.54%)\n",
      "\t0.6899\t = Validation score   (accuracy)\n",
      "\t2.74s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 12.63s of the 22.4s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.71%)\n",
      "\t0.6725\t = Validation score   (accuracy)\n",
      "\t2.67s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 5.56s of the 15.34s of remaining time.\n",
      "\t0.6107\t = Validation score   (accuracy)\n",
      "\t1.06s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 4.24s of the 14.01s of remaining time.\n",
      "\t0.6282\t = Validation score   (accuracy)\n",
      "\t1.07s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2.9s of the 12.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.08%)\n",
      "\t0.6819\t = Validation score   (accuracy)\n",
      "\t2.43s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.37s of the 5.66s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.6899\t = Validation score   (accuracy)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 4.84s of the 4.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.56%)\n",
      "\t0.6819\t = Validation score   (accuracy)\n",
      "\t2.42s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 29.37s of the -3.28s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.6899\t = Validation score   (accuracy)\n",
      "\t0.83s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 42.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t0.56s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t0.52s\t = Training   runtime\n",
      "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t1.06s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t1.07s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t1.04s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.8s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\t0.49s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.83s\t = Training   runtime\n",
      "Updated best model to \"LightGBMXT_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBMXT_BAG_L1_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 2.97s ... Best model: \"LightGBMXT_BAG_L1_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240410_200843\")\n"
     ]
    }
   ],
   "source": [
    "time_limit = 60\n",
    "predictor1 = TabularPredictor(label='Performance').fit(train_data, time_limit=time_limit, presets='high_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a95ae-f22a-40c1-a3e5-49b9c2ada874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c5a53-7861-4f5c-84ff-b1e061ee5617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
